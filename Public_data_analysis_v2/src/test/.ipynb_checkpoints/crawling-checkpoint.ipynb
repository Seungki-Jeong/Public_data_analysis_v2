{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경향신문 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경향신문 크롤링\n",
    "\n",
    "def kh():\n",
    "    # 경향신문 '천안문 검색 결과' URL\n",
    "    url = 'http://search.khan.co.kr/search.html?stb=khan&q=%EC%B2%9C%EC%95%88%EB%AC%B8&pg={}&sort=1'\n",
    "\n",
    "    # 가져올 텍스트 공간 초기화\n",
    "    global kh_word\n",
    "    kh_word = []\n",
    "\n",
    "    # 경향신문 1~10페이지까지 크롤링 (최대 10페이지까지 존재)\n",
    "    for count in tqdm(range(1, 11)):\n",
    "\n",
    "    #     1~10페이지까지 html을 가져옴\n",
    "        response = requests.get(url.format(count))\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #     title 부분\n",
    "        titles = soup.select('.phArtc > dt > a')\n",
    "    #     소제목 부분\n",
    "        words = soup.select('.phArtc > .txt')\n",
    "\n",
    "    #     타이틀과 소제목을 모두 합침\n",
    "        for tit, wor in zip(titles, words):\n",
    "            kh_word.append(tit.text)\n",
    "            kh_word.append(wor.text)\n",
    "\n",
    "#     크롤링한 텍스트 데이터 저장\n",
    "    with open('../data/kh.txt', mode = 'wt', encoding = 'utf-8') as file:\n",
    "        for w in kh_word:\n",
    "            file.writelines('{}\\n'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동아일보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동아일보 크롤링\n",
    "\n",
    "def donga():\n",
    "    # 동아일보 '천안문 검색 결과' URL\n",
    "    url = 'https://www.donga.com/news/search?p={}&query=%EC%B2%9C%EC%95%88%EB%AC%B8&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'\n",
    "\n",
    "    # 가져올 텍스트 공간 초기화\n",
    "    global donga_word\n",
    "    donga_word = []\n",
    "\n",
    "    # 동아일보 1~1051페이지까지 크롤링 (최대 10페이지까지 존재)\n",
    "    for count in tqdm(range(1, 1052)):\n",
    "    #     1~1051페이지까지 html을 가져옴\n",
    "        response = requests.get(url.format(count))\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #     title 부분\n",
    "        titles = soup.select('.searchList > .t > .tit > a')\n",
    "    #     소제목 부분\n",
    "        words = soup.select('.searchList > .t > .txt > a')\n",
    "\n",
    "    #     타이틀과 소제목을 모두 합침\n",
    "        for tit, wor in zip(titles, words):\n",
    "            donga_word.append(tit.text)\n",
    "            donga_word.append(wor.text)\n",
    "\n",
    "    #     크롤링한 텍스트 데이터 저장\n",
    "    with open('../data/donga.txt', mode = 'wt', encoding = 'utf-8') as file:\n",
    "        for w in donga_word:\n",
    "            filechosun_wordlines('{}\\n'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한겨레 신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한겨레 크롤링\n",
    "\n",
    "def hani():\n",
    "    # 한겨레 '천안문 검색 결과' URL\n",
    "    url = 'http://search.hani.co.kr/Search?command=query&keyword=%EC%B2%9C%EC%95%88%EB%AC%B8&media=news&submedia=&sort=d&period=all&datefrom=1988.01.01&dateto=2020.09.22&pageseq={}'\n",
    "\n",
    "    # 가져올 텍스트 공간 초기화\n",
    "    global hani_word\n",
    "    hani_word = []\n",
    "\n",
    "    # 동아일보 1~1051페이지까지 크롤링 (최대 10페이지까지 존재)\n",
    "    for count in tqdm(range(0, 78)):\n",
    "    #     1~1051페이지까지 html을 가져옴\n",
    "        response = requests.get(url.format(count))\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #     title 부분\n",
    "        titles = soup.select('.search-result-list > li > dl > dt > a')\n",
    "    #     소제목 부분\n",
    "        words = soup.select('.search-result-list > li > dl > .detail')\n",
    "\n",
    "    #     타이틀과 소제목을 모두 합침\n",
    "        for tit, wor in zip(titles, words):\n",
    "            hani_word.append(tit.text)\n",
    "            hani_word.append(wor.text)\n",
    "\n",
    "    #     크롤링한 텍스트 데이터 저장\n",
    "    with open('../data/hani.txt', mode = 'wt', encoding = 'utf-8') as file:\n",
    "        for w in hani_word:\n",
    "            file.writelines('{}\\n'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조선일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조선일보 크롤링\n",
    "\n",
    "def chosun():\n",
    "    \n",
    "#     가져올 텍스트 공간 초기화\n",
    "    global chosun_word\n",
    "    chosun_word = []\n",
    "\n",
    "#     selenium로 웹페이지 제어하고 ULR 접속\n",
    "    driver = webdriver.Chrome('../data/chromedriver')\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.get('https://www.chosun.com/search/query=%EC%B2%9C%EC%95%88%EB%AC%B8/')\n",
    "\n",
    "#     웹페이지에서 더보기 페이지 전부 펴기\n",
    "    while True:\n",
    "        try:\n",
    "            driver.find_element_by_id('load-more-stories').click()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "#     BeautifulSoup 크롤링을 위해 page_sorce 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#     title 부분\n",
    "    titles = soup.select('.story-card-wrapper > .story-card > .story-card-right > .story-card__headline-container > h3 > a > span')\n",
    "#     소제목 부분\n",
    "    words = soup.select('.story-card-wrapper > .story-card > .story-card-right > .story-card__deck > span')\n",
    "\n",
    "#      타이틀과 소제목을 합침\n",
    "    for tit, wor in zip(titles, words):\n",
    "            chosun_word.append(tit.text)\n",
    "            chosun_word.append(wor.text)\n",
    "\n",
    "    #     크롤링한 텍스트 데이터 저장\n",
    "    with open('../data/chosun.txt', mode = 'wt', encoding = 'utf-8') as file:\n",
    "        for w in chosun_word:\n",
    "            file.writelines('{}\\n'.format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
